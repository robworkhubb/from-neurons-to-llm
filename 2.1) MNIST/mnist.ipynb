{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94131d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80052586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertire immagini MNIST in tensori di 4 dimensioni (n di immagini, altezza, larghezza e canali)\n",
    "transform = transforms.ToTensor()\n",
    "# Dati di training\n",
    "train_data = datasets.MNIST(root = 'cnn_data', train = True, download=True, transform=transform)\n",
    "# Dati di test\n",
    "test_data = datasets.MNIST(root = 'cnn_data', train = False, download=True, transform=transform)\n",
    "# Dataloader\n",
    "train_loader = DataLoader(train_data, batch_size=10, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59662c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiamo la CNN\n",
    "conv1 = nn.Conv2d(1, 6, 3, 1)\n",
    "conv2 = nn.Conv2d(6, 16, 3, 1)\n",
    "# Prendiamo 1 immagine MNIST\n",
    "for i, (x_train, y_train) in enumerate(train_data):\n",
    "    break\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4daa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertire l'immagine in 4D\n",
    "x = x_train.view(1, 1,28,28)\n",
    "# Prima convoluzione\n",
    "x = F.relu(conv1(x)) # Funzione di Attivazione\n",
    "# x.shape OUTPUT: 1,6,26,26: 1 è l'immagine singola, \n",
    "# 6 i filtri che abbiamo chiesto (riga 24),\n",
    "# 26*26 è l'immagine.\n",
    "# Pooling layer\n",
    "x = F.max_pool2d(x, 2, 2) # Kernel size 2 e stride 2 \n",
    "x = F.relu(conv2(x))\n",
    "x = F.max_pool2d(x, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b97c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe del modello\n",
    "class ConvolutionalNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3, 1)\n",
    "        # Fully connected layer\n",
    "        self.fc1 = nn.Linear(5*5*16, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2) # 2 x 2 Kernel e Stride 2\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        \n",
    "        x = x.view(-1, 16*5*5) # Numero negativo cosi possiamo variare la dimensione del batch\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "# Creiamo un istanza del modello\n",
    "torch.manual_seed(41)\n",
    "model = ConvolutionalNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef80362f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss FN Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "# Allenamento\n",
    "# Variabili\n",
    "epochs = 5\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_correct = []\n",
    "test_correct = []\n",
    "\n",
    "# Loop\n",
    "for i in range(epochs):\n",
    "    trn_corr = 0\n",
    "    tst_corr = 0\n",
    "    # train\n",
    "    for b,(x_train, y_train) in enumerate(train_loader):\n",
    "        b += 1 # Inizializziamo i nostri batches a 1\n",
    "        y_pred = model(x_train) # Prendiamo la y iniziale dal nostro set di addestramento\n",
    "        loss = criterion(y_pred, y_train) # Confrontiamo la predizione con le risposte corrette in y_train\n",
    "        \n",
    "        predicted = torch.max(y_pred.data, 1)[1] # Aggiungere il numero delle predizioni corrette. Indicizzato al primo punto\n",
    "        batch_corr = (predicted == y_train).sum() # Quando è stato corretto dal suo batch. True = 1, False = 0\n",
    "        trn_corr += batch_corr # Tiene traccia mentre procediamo con l'allenamento\n",
    "        # Aggiornamento Parametri\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        # Print\n",
    "        if b%600 == 0:\n",
    "            print(f'Epoch: {i} Batch: {b} Loss: {loss.item()}')\n",
    "    train_losses.append(loss)\n",
    "    train_correct.append(trn_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec854b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test\n",
    "with torch.no_grad(): # No gradienti perché non vogliamo aggiornare i pesi nel test\n",
    "    for b,(x_test, y_test) in enumerate(test_loader):\n",
    "        y_val = model(x_test)\n",
    "        predicted = torch.max(y_val.data, 1)[1] # Aggiunge predizioni corrette\n",
    "        tst_corr += (predicted == y_test).sum()\n",
    "loss = criterion(y_val, y_test)\n",
    "test_losses.append(loss)\n",
    "test_correct.append(tst_corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "f6051e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x23e02d7c920>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_idx = 5214\n",
    "# Prendiamo un'immagine\n",
    "test_data[img_idx] # tensore con un immagine e mostrerà il label \n",
    "# Prendiamone solo i dati\n",
    "test_data[img_idx][0]\n",
    "# Ridimensioniamo\n",
    "test_data[img_idx][0].reshape(28, 28)\n",
    "# Mostrare\n",
    "plt.imshow(test_data[img_idx][0].reshape(28, 28)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "b9e6d264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-21.9002,  -7.2020,  -7.7074,  11.6253, -16.8083,  -3.5839, -21.1401,\n",
       "          -7.0264, -10.0031,  -8.9535]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Passiamo l'immagine al model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    new_pred = model(test_data[img_idx][0].view(1,1,28,28)) #batch_size di 1, 1 canale dei colori, dimensione 28*28\n",
    "# probabilità \n",
    "new_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "98463c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_pred.argmax()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
