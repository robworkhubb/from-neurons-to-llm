{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d773b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tiktoken\n",
    "config = {\n",
    "    \"embed_dim\": 128,\n",
    "    \"num_heads\": 8,\n",
    "    \"max_len\": 100,\n",
    "    \"num_classes\": 1,\n",
    "    \"batch_size\": 2,\n",
    "    \"num_epochs\": 5,\n",
    "    \"lr\": 0.001\n",
    "}\n",
    "encoder = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9f9a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "\n",
    "with open(\"train_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "for item in data:\n",
    "    texts.append(item[\"text\"])\n",
    "    labels.append(item[\"label\"])\n",
    "\n",
    "inputs = []\n",
    "targets = []\n",
    "\n",
    "for text, label in zip(texts, labels): # Accoppia le liste zip\n",
    "    tokens = encoder.encode(text)\n",
    "    if len(tokens) < config[\"max_len\"]:\n",
    "        tokens += [0] * (config[\"max_len\"] - len(tokens))\n",
    "    else:\n",
    "        tokens = tokens[:config[\"max_len\"]]\n",
    "    inputs.append(tokens)\n",
    "    targets.append(labels)\n",
    "\n",
    "inputs = torch.tensor(inputs, dtype=torch.long)\n",
    "targets = torch.tensor(labels, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920dc0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2 # Livello di blocchi\n",
    "dataset_size = inputs.size(0)\n",
    "\n",
    "indices = torch.randperm(dataset_size)\n",
    "shuffled_inputs = inputs[indices]\n",
    "shuffled_targets = targets[indices]\n",
    "\n",
    "for start_idx in range(0, dataset_size, batch_size):\n",
    "    end_idx = start_idx + batch_size\n",
    "    batch_inputs = shuffled_inputs[start_idx:end_idx]\n",
    "    batch_targets = shuffled_targets[start_idx:end_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "708c04fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, max_len = 100):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(encoder.n_vocab, embed_dim)\n",
    "        \n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len, embed_dim))\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model = embed_dim,\n",
    "            nhead = num_heads,\n",
    "            batch_first = True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers = 1)\n",
    "        \n",
    "        self.fc = nn.Linear(embed_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        seq_len = x.size(1)\n",
    "        \n",
    "        x = x + self.pos_embedding[:, :seq_len, :]\n",
    "        \n",
    "        x = self.transformer_encoder(x)\n",
    "        \n",
    "        x = x.mean(dim=1)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = Model(\n",
    "    encoder.n_vocab,\n",
    "    config[\"embed_dim\"],\n",
    "    config[\"num_heads\"],\n",
    "    config[\"max_len\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be893f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.6868\n",
      "Epoch 2/20, Loss: 0.6552\n",
      "Epoch 3/20, Loss: 0.5603\n",
      "Epoch 4/20, Loss: 0.2165\n",
      "Epoch 5/20, Loss: 0.0920\n",
      "Epoch 6/20, Loss: 0.2450\n",
      "Epoch 7/20, Loss: 0.0115\n",
      "Epoch 8/20, Loss: 0.0076\n",
      "Epoch 9/20, Loss: 0.0062\n",
      "Epoch 10/20, Loss: 0.0056\n",
      "Epoch 11/20, Loss: 0.0047\n",
      "Epoch 12/20, Loss: 0.0040\n",
      "Epoch 13/20, Loss: 0.0043\n",
      "Epoch 14/20, Loss: 0.0046\n",
      "Epoch 15/20, Loss: 0.0034\n",
      "Epoch 16/20, Loss: 0.0035\n",
      "Epoch 17/20, Loss: 0.0029\n",
      "Epoch 18/20, Loss: 0.0037\n",
      "Epoch 19/20, Loss: 0.0027\n",
      "Epoch 20/20, Loss: 0.0028\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    indices = torch.randperm(dataset_size)\n",
    "    shuffled_inputs = inputs[indices]\n",
    "    shuffled_targets = targets[indices]\n",
    "\n",
    "    for start_idx in range(0, dataset_size, batch_size):\n",
    "        end_idx = start_idx + batch_size\n",
    "        batch_inputs = shuffled_inputs[start_idx:end_idx]\n",
    "        batch_targets = shuffled_targets[start_idx:end_idx]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_inputs)\n",
    "        loss = criterion(outputs.squeeze(), batch_targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1142e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predizione: Positivo, Probabilità: 0.5992962718009949\n"
     ]
    }
   ],
   "source": [
    "new_sentence = \"Il concerto è stato bellissimo\"\n",
    "tokens = encoder.encode(new_sentence)\n",
    "\n",
    "input_tensor = torch.tensor([tokens], dtype=torch.long)\n",
    "model.eval()  \n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "    prob = torch.sigmoid(output)\n",
    "if prob.item() > 0.5:\n",
    "    print(\"Predizione: Positivo, Probabilità:\", prob.item())\n",
    "else:\n",
    "    print(\"Predizione: Negativo, Probabilità:\", prob.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
